# Example showing inference with vLLM

This folder contains multiple demonstrations showcasing the integration of [vLLM Engine](https://github.com/vllm-project/vllm) with TorchServe, running inference on `mistralai/Mistral-7B-v0.1` model and multiple LoRA models.
vLLM achieves high throughput using PagedAttention. More details can be found [here](https://vllm.ai/)

- demo1: [Mistral](mistral)
- demo2: [lora](lora)
