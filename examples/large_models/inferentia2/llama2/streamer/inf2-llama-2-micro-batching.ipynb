{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TorchServe Continuous Batching Serve Llama-2-70B on Inferentia-2\n",
    "This notebook demonstrates TorchServe stream with microbatching serving Llama-2-70b on Inferentia-2 `inf2.48xlarge` with Neuron DLAMI Deep Learning AMI Neuron (Ubuntu 22.04) 20240401 and Neuron DLC [public.ecr.aws/neuron/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.18.0-ubuntu20.04](https://github.com/aws-neuron/deep-learning-containers?tab=readme-ov-file#pytorch-inference-neuronx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Activate Transformers NeuronX (PyTorch 2.1) Python venv\n",
    "!source /opt/aws_neuronx_venv_transformers_neuronx/bin/activate\n",
    "\n",
    "# Install torch-model-archiver\n",
    "!pip install torch-model-archiver\n",
    "\n",
    "# Clone Torchserve git repository\n",
    "!git clone https://github.com/pytorch/serve.git\n",
    "\n",
    "# Install dependencies, now all commands run under serve dir\n",
    "!cd serve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create model artifacts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# login in Hugginface hub\n",
    "!pip install --upgrade huggingface_hub\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN\n",
    "!python examples/large_models/utils/Download_model.py --model_path model --model_name meta-llama/Llama-2-70b-hf --use_auth_token True\n",
    "\n",
    "# Create TorchServe model artifacts\n",
    "!torch-model-archiver --model-name llama-2-70b --version 1.0 --handler ts/torch_handler/distributed/base_neuronx_microbatching_handler.py -r examples/large_models/inferentia2/llama2/streamer/requirements.txt --config-file examples/large_models/inferentia2/llama2/streamer/model-config.yaml --archive-format no-archive\n",
    "\n",
    "!mkdir model_store\n",
    "!mv llama-2-70b model_store\n",
    "!mv model model_store/llama-2-70b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Start docker"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!docker run -it -v /home/ubuntu/serve/model_store/:/home/model-server/model_store -v /home/ubuntu/serve/:/home/model-server/serve --device /dev/neuron0:/dev/neuron0  --device /dev/neuron1:/dev/neuron1  --device /dev/neuron2:/dev/neuron2  --device /dev/neuron3:/dev/neuron3  --device /dev/neuron4:/dev/neuron4  --device /dev/neuron5:/dev/neuron5 --device /dev/neuron0:/dev/neuron6 --device /dev/neuron1:/dev/neuron7  --device /dev/neuron2:/dev/neuron8  --device /dev/neuron3:/dev/neuron9  --device /dev/neuron4:/dev/neuron10  --device /dev/neuron5:/dev/neuron11 -p 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071  public.ecr.aws/neuron/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.18.0-ubuntu20.04 bash"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Inside container"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torchserve==0.10.0\n",
    "!cd /home/model-server/serve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start TorchServe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1315009035.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    torchserve --ncs --start --model-store model_store --models llama-2-70b --ts-config examples/large_models/inferentia2/llama2/config.properties\u001B[0m\n\u001B[0m                                           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!torchserve --ncs --start --model-store model_store --models llama-2-70b --ts-config examples/large_models/inferentia2/llama2/config.properties"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run single inference request\n",
    "!python examples/large_models/utils/test_llm_streaming_response.py -m llama-2-70b -o 50 -t 2 -n 4 --prompt-text \"Today the weather is really nice and I am planning on \""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
